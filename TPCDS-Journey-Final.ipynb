{
    "cells": [
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "## Get the journey data.\nClones the tpcds journey repository to get access to all the data and scripts that are required to excercise this journey. Normally the data and queries are generated by running the data and query generation utility from the tpcds toolkit available at http://www.tpc.org/tpcds. However for easy of use, the data and queries are pre-generated for 1GB scale factor. We use the pre-generated data and queries to demonstrate how they can be used to run the tpcds queries against spark.\n"
        }, 
        {
            "cell_type": "code", 
            "outputs": [
                {
                    "text": "Starting download from https://brunelvis.org/jar/spark-kernel-brunel-all-2.4.jar\nFinished download of spark-kernel-brunel-all-2.4.jar\ngit version 1.8.3.1\nCloning into 'tpcds-journey'...\nremote: Counting objects: 1037, done.        \nremote: Total 1037 (delta 0), reused 0 (delta 0), pack-reused 1037        \nReceiving objects: 100% (1037/1037), 362.46 MiB | 10.05 MiB/s, done.\nResolving deltas: 100% (453/453), done.\nChecking out files: 100% (806/806), done.\n", 
                    "output_type": "stream", 
                    "name": "stdout"
                }
            ], 
            "metadata": {}, 
            "source": "import sys.process._\n%AddJar -magic https://brunelvis.org/jar/spark-kernel-brunel-all-2.4.jar\n\"rm -rf tpcds-journey\" !\n\"git --version\" !\n\"git clone --progress https://github.com/SparkTC/tpcds-journey.git\" !", 
            "execution_count": 1
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "## Setup variables.\n* Sets up variables that are used in the rest of this notebook.\n* The path variables are relative to the git clone directory.\n* tpcdsDatabaseName is hard-coded to \"TPCDS1G\". This can be changed if a different database name is desired."
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "def deleteFile1(tableName: String): Unit = {\n    import sys.process._\n    val commandStr1 = s\"rm -rf spark-warehouse/tpcds2g.db/${tableName}/*\"\n    val commandStr2 = s\"rm -rf spark-warehouse/tpcds2g.db/${tableName}\"\n    commandStr1 !\n}", 
            "execution_count": 2
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "def deleteFile2(tableName: String): Unit = {\n    import sys.process._\n    val commandStr2 = s\"rm -rf spark-warehouse/tpcds2g.db/${tableName}\"\n    commandStr2 !\n}", 
            "execution_count": 3
        }, 
        {
            "cell_type": "code", 
            "outputs": [
                {
                    "text": "TPCDS root directory is at : tpcds-journey\nTPCDS ddl scripts directory is at: tpcds-journey/src/ddl/individual\nTPCDS data directory is at: tpcds-journey/src/data\nTPCDS queries directory is at: tpcds-journey/src/queries\n", 
                    "output_type": "stream", 
                    "name": "stdout"
                }
            ], 
            "metadata": {}, 
            "source": "val tpcdsRootDir = \"tpcds-journey\"\nval tpcdsWorkDir = \"tpcds-journey/work\"\nval tpcdsDdlDir = s\"${tpcdsRootDir}/src/ddl/individual\"\nval tpcdsGenDataDir = s\"${tpcdsRootDir}/src/data\"\nval tpcdsQueriesDir = s\"${tpcdsRootDir}/src/queries\"\nval tpcdsDatabaseName = \"TPCDS2G\"\nvar totalTime: Long = 0\nprintln(\"TPCDS root directory is at : \"+ tpcdsRootDir)\nprintln(\"TPCDS ddl scripts directory is at: \" + tpcdsDdlDir)\nprintln(\"TPCDS data directory is at: \"+ tpcdsGenDataDir)\nprintln(\"TPCDS queries directory is at: \"+ tpcdsQueriesDir)\n\nval journey_spark = SparkSession.\n    builder().\n    config(\"spark.ui.showConsoleProgress\", false).\n    config(\"spark.sql.autoBroadcastJoinThreshold\", -1).\n    config(\"spark.sql.crossJoin.enabled\", true).\n    getOrCreate()\n\n\njourney_spark.sparkContext.setLogLevel(\"ERROR\")", 
            "execution_count": 4
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "## Setup the TPC-DS schema\n* Creates the database as specified by tpcdsDatabaseName\n* Creates all the tpc-ds tables.\n* Loads data into the tables in parquet format. \n  * Since the data generated by tpc-ds toolkit is in CSV format, we do the loading in multi steps.\n  * As first step, we create tables in csv format by pointing the location to the generated data.\n  * As second step, we create parquet tables by using CTAS that convert text data into parquet.\n  * As last step, we drop the text tables as we longer need them."
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### Utility function definitions.\n* Defines the utility functions that are called from the cells below in the notebook."
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }, 
            "source": "def createDatabase(): Unit = {\n    journey_spark.sql(s\"DROP DATABASE IF EXISTS ${tpcdsDatabaseName} CASCADE\")\n    journey_spark.sql(s\"CREATE DATABASE ${tpcdsDatabaseName}\")\n    journey_spark.sql(s\"USE ${tpcdsDatabaseName}\")\n}\n\n/**\n * Function to create a table in spark. It reads the DDL script for each of the\n * tpc-ds table and executes it on Spark.\n */\ndef createTable(tableName: String): Unit = {\n  println(s\"Creating table $tableName ..\")\n  journey_spark.sql(s\"DROP TABLE IF EXISTS $tableName\")\n  deleteFile1(tableName)   \n  deleteFile2(tableName)    \n  val (fileName, content) = \n    journey_spark.sparkContext.wholeTextFiles(s\"${tpcdsDdlDir}/$tableName.sql\").collect()(0) \n    \n  // Remove the replace for the .dat once it is fixed in the github repo\n  val sqlStmts = content.stripLineEnd\n    .replace('\\n', ' ')\n    .replace(\"${TPCDS_GENDATA_DIR}\", tpcdsGenDataDir)\n    .replace(\"csv\", \"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\").split(\";\")\n  sqlStmts.map(stmt => journey_spark.sql(stmt))    \n}  \n\nimport scala.collection.mutable.ArrayBuffer\nimport org.apache.spark.sql.DataFrame\n\ndef runQuery(queryStr: String,\n             individual: Boolean = true,\n             resultDir: String): Seq[(String, Double, Int, String)] = {\n  val querySummary = ArrayBuffer.empty[(String, Double, Int, String)]  \n  val queryName = s\"${tpcdsQueriesDir}/query${queryStr}.sql\"   \n  val (_, content) = journey_spark.sparkContext.wholeTextFiles(queryName).collect()(0)  \n  val queries = content.split(\"\\n\")\n    .filterNot (_.startsWith(\"--\"))\n    .mkString(\" \").split(\";\")\n  \n  var cnt = 1  \n  for (query <- queries)  {\n   val start = System.nanoTime()\n   val df = journey_spark.sql(query)   \n   val result = journey_spark.sql(query).collect  \n   val timeElapsed = (System.nanoTime() - start) / 1000000000\n   val name = if (queries.length > 1) {\n       s\"query${queryStr}-${cnt}\"\n   } else {\n       s\"query${queryStr}\"\n   }  \n   val resultFile = s\"${resultDir}/${name}-notebook.res\"  \n   df.coalesce(1)\n      .write.format(\"com.databricks.spark.csv\")\n      .option(\"header\", \"true\")\n      .mode(\"overwrite\")\n      .save(resultFile)\n   totalTime = totalTime + timeElapsed\n  \n   querySummary += Tuple4.apply(name, timeElapsed, result.length, resultFile)\n   cnt += 1                \n  }\n  querySummary \n}\n\n// run function for each table in tables array\ndef forEachTable(tables: Array[String], f: (String) => Unit): Unit = {\n  for ( table <- tables) {\n    try {\n      f(table)\n    } catch {\n      case e: Throwable => {\n        println(\"EXCEPTION!! \" + e.getMessage())\n        throw e\n      }\n    }\n  }\n}\n\ndef runIndividualQuery(queryNum: Int, resultDir: String = tpcdsWorkDir ): DataFrame = {\n    val queryStr = \"%02d\".format(queryNum) \n    val testSummary = ArrayBuffer.empty[(String, Double, Int, String)] \n    try {      \n      println(s\"Running TPC-DS Query : $queryStr\")  \n      testSummary ++= runQuery(queryStr, true, resultDir)\n    } catch {\n        case e: Throwable => {\n            println(\"Error in query \"+ queryNum + \"msg = \" + e.getMessage)\n        }\n    }\n    testSummary.toDF(\"QueryName\",\"ElapsedTime\",\"RowsReturned\", \"ResultLocation\")\n}\n\ndef runAllQueries(resultDir: String = tpcdsWorkDir): DataFrame = {\n  val testSummary = ArrayBuffer.empty[(String, Double, Int, String)]    \n  var queryErrors = 0\n  for (i <- 1 to 99) {\n    try{\n      val queryStr = \"%02d\".format(i)\n      println(s\"Running TPC-DS Query : $queryStr\")   \n      testSummary ++= runQuery(queryStr, false, resultDir)\n    } catch {\n       case e: Throwable => {\n            println(\"Error in query \"+ i + \"msg = \" + e.getMessage)\n            queryErrors += 1\n       }\n    }\n  }\n\n  println(\"=====================================================\")\n  if ( queryErrors > 0) {\n    println(s\"Query execution failed with $queryErrors errors\")\n  } else {\n    println(\"All TPC-DS queries ran successfully\")\n  }\n  println (s\"Total Elapsed Time so far: ${totalTime} seconds.\")\n  println(\"=====================================================\")\n  testSummary.toDF(\"QueryName\",\"ElapsedTime\",\"RowsReturned\", \"ResultLocation\")\n}\n\ndef displaySummary(summaryDF: DataFrame): Unit = {\n    summaryDF.show(10000)\n}\n\ndef displayResult(queryNum: Int, summaryDF: DataFrame) = {\n   val queryStr = \"%02d\".format(queryNum)\n   // Find result files for this query number. For some queries there are\n   // multiple result files. \n   val  files = summaryDF.where(s\"queryName like 'query${queryStr}%'\").select(\"ResultLocation\").collect()\n   for (file <- files) {\n       val fileName = file.getString(0)\n       val df = journey_spark.read\n         .format(\"csv\")\n         .option(\"header\", \"true\") //reading the headers\n         .option(\"mode\", \"DROPMALFORMED\")\n         .load(fileName)\n       val numRows:Int = df.count().toInt\n       df.show(numRows, truncate=false)\n   }\n}\n\ndef explainQuery(queryNum: Int) = {\n  val queryStr = \"%02d\".format(queryNum)  \n  val queryName = s\"${tpcdsQueriesDir}/query${queryStr}.sql\"   \n  val (_, content) = journey_spark.sparkContext.wholeTextFiles(queryName).collect()(0)  \n  val queries = content.split(\"\\n\")\n    .filterNot (_.startsWith(\"--\"))\n    .mkString(\" \").split(\";\")\n    \n  for (query <- queries)  {    \n    journey_spark.sql(query).explain(true) \n  }\n}", 
            "execution_count": 5
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### Create the database and tables.\n* Creates the tpc-ds database.\n* For each of the table name in TPC-DS schema, calls up on the function to create the table in spark."
        }, 
        {
            "cell_type": "code", 
            "outputs": [
                {
                    "text": "Creating table call_center ..\nCreating table catalog_sales ..\nCreating table customer_demographics ..\nCreating table income_band ..\nCreating table promotion ..\nCreating table store ..\nCreating table time_dim ..\nCreating table web_returns ..\nCreating table catalog_page ..\nCreating table customer ..\nCreating table date_dim ..\nCreating table inventory ..\nCreating table reason ..\nCreating table store_returns ..\nCreating table warehouse ..\nCreating table web_sales ..\nCreating table catalog_returns ..\nCreating table customer_address ..\nCreating table household_demographics ..\nCreating table item ..\nCreating table ship_mode ..\nCreating table store_sales ..\nCreating table web_page ..\nCreating table web_site ..\n", 
                    "output_type": "stream", 
                    "name": "stdout"
                }
            ], 
            "metadata": {}, 
            "source": "// TPC-DS table names.\nval tables = Array(\"call_center\", \"catalog_sales\",\n                   \"customer_demographics\", \"income_band\",\n                   \"promotion\", \"store\", \"time_dim\", \"web_returns\",\n                   \"catalog_page\", \"customer\", \"date_dim\",\n                   \"inventory\", \"reason\", \"store_returns\", \"warehouse\",\n                   \"web_sales\", \"catalog_returns\", \"customer_address\",\n                   \"household_demographics\", \"item\", \"ship_mode\", \"store_sales\",\n                   \"web_page\", \"web_site\" )\n\n// Create database\ncreateDatabase\n\n// Create table\nforEachTable(tables, table => createTable(table))\n", 
            "execution_count": 6
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "## Verify table creating and data loading.\n* Run a simple Spark SQL query to get the count of rows\n* Verify that the row counts are as expected"
        }, 
        {
            "cell_type": "code", 
            "outputs": [
                {
                    "text": "=====================================================\nLoaded and verified the table counts successfully\n=====================================================\n", 
                    "output_type": "stream", 
                    "name": "stdout"
                }
            ], 
            "metadata": {}, 
            "source": "// Run a count query and get the counts\nval rowCounts = tables.map { table =>\n    journey_spark.table(table).count()\n}\n\nval expectedCounts = Array (\n    6, 1441548, 1920800, 20, 300, 12, 86400,\n    71763,  11718, 100000, 73049, 11745000, \n    35, 287514, 5, 719384, 144067, 50000, 7200,\n    18000, 20, 2880404, 60, 30\n)\n\nvar errorCount = 0;\nval zippedCountsWithIndex = rowCounts.zip(expectedCounts).zipWithIndex\nfor ((pair, index) <- zippedCountsWithIndex) {\n    if (pair._1 != pair._2) {\n        println(s\"\"\"ERROR!! Row counts for ${tables(index)} does not match.\n        Expected=${expectedCounts(index)} but found ${rowCounts(index)}\"\"\")\n        errorCount += 1\n    }\n}\n\nprintln(\"=====================================================\")\nif ( errorCount > 0) {\n  println(s\"Load verification failed with $errorCount errors\")\n} else {\n  println(\"Loaded and verified the table counts successfully\")\n}\nprintln(\"=====================================================\")", 
            "execution_count": 7
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "## Run a single query\n* Given a query number between 1 to 99, run it against spark.\n* Display the query results, time taken to execute the query and number of rows returned.\n* To run a differnt query , please change the QUERY_NUM to a valid value from 1 to 99."
        }, 
        {
            "cell_type": "code", 
            "outputs": [
                {
                    "text": "Running TPC-DS Query : 01\nsingle query path\n+---------+-----------+------------+--------------------+\n|QueryName|ElapsedTime|RowsReturned|      ResultLocation|\n+---------+-----------+------------+--------------------+\n|  query01|       12.0|         100|tpcds-journey/wor...|\n+---------+-----------+------------+--------------------+\n\n+----------------+\n|c_customer_id   |\n+----------------+\n|AAAAAAAAAAABBAAA|\n|AAAAAAAAAAADBAAA|\n|AAAAAAAAAAADBAAA|\n|AAAAAAAAAAAKAAAA|\n|AAAAAAAAAABDAAAA|\n|AAAAAAAAAABHBAAA|\n|AAAAAAAAAABLAAAA|\n|AAAAAAAAAABMAAAA|\n|AAAAAAAAAACHAAAA|\n|AAAAAAAAAACMAAAA|\n|AAAAAAAAAADDAAAA|\n|AAAAAAAAAADGAAAA|\n|AAAAAAAAAADGBAAA|\n|AAAAAAAAAADGBAAA|\n|AAAAAAAAAADPAAAA|\n|AAAAAAAAAAEBAAAA|\n|AAAAAAAAAAEFBAAA|\n|AAAAAAAAAAEGBAAA|\n|AAAAAAAAAAEIAAAA|\n|AAAAAAAAAAEMAAAA|\n|AAAAAAAAAAFAAAAA|\n|AAAAAAAAAAFPAAAA|\n|AAAAAAAAAAGGBAAA|\n|AAAAAAAAAAGHBAAA|\n|AAAAAAAAAAGJAAAA|\n|AAAAAAAAAAGMAAAA|\n|AAAAAAAAAAHEBAAA|\n|AAAAAAAAAAHFBAAA|\n|AAAAAAAAAAIEBAAA|\n|AAAAAAAAAAJGBAAA|\n|AAAAAAAAAAJHBAAA|\n|AAAAAAAAAAKCAAAA|\n|AAAAAAAAAAKCAAAA|\n|AAAAAAAAAAKJAAAA|\n|AAAAAAAAAAKMAAAA|\n|AAAAAAAAAAKMAAAA|\n|AAAAAAAAAALAAAAA|\n|AAAAAAAAAALABAAA|\n|AAAAAAAAAALGAAAA|\n|AAAAAAAAAALHBAAA|\n|AAAAAAAAAALJAAAA|\n|AAAAAAAAAANHAAAA|\n|AAAAAAAAAANHBAAA|\n|AAAAAAAAAANJAAAA|\n|AAAAAAAAAANMAAAA|\n|AAAAAAAAAANMAAAA|\n|AAAAAAAAAANNAAAA|\n|AAAAAAAAAAOBBAAA|\n|AAAAAAAAAAODBAAA|\n|AAAAAAAAAAOLAAAA|\n|AAAAAAAAAAPGBAAA|\n|AAAAAAAAABAAAAAA|\n|AAAAAAAAABAEAAAA|\n|AAAAAAAAABAEBAAA|\n|AAAAAAAAABAFBAAA|\n|AAAAAAAAABAIAAAA|\n|AAAAAAAAABAOAAAA|\n|AAAAAAAAABBDBAAA|\n|AAAAAAAAABCFAAAA|\n|AAAAAAAAABCHBAAA|\n|AAAAAAAAABDHAAAA|\n|AAAAAAAAABENAAAA|\n|AAAAAAAAABFEBAAA|\n|AAAAAAAAABFGAAAA|\n|AAAAAAAAABFMAAAA|\n|AAAAAAAAABFPAAAA|\n|AAAAAAAAABGFAAAA|\n|AAAAAAAAABGFBAAA|\n|AAAAAAAAABGJAAAA|\n|AAAAAAAAABIBBAAA|\n|AAAAAAAAABICBAAA|\n|AAAAAAAAABIIAAAA|\n|AAAAAAAAABJNAAAA|\n|AAAAAAAAABKGBAAA|\n|AAAAAAAAABLOAAAA|\n|AAAAAAAAABLPAAAA|\n|AAAAAAAAABMABAAA|\n|AAAAAAAAABMPAAAA|\n|AAAAAAAAABNAAAAA|\n|AAAAAAAAABNCBAAA|\n|AAAAAAAAABNEBAAA|\n|AAAAAAAAABNLAAAA|\n|AAAAAAAAABNOAAAA|\n|AAAAAAAAABNPAAAA|\n|AAAAAAAAABOAAAAA|\n|AAAAAAAAABOFBAAA|\n|AAAAAAAAABOOAAAA|\n|AAAAAAAAABOPAAAA|\n|AAAAAAAAABPEAAAA|\n|AAAAAAAAACADAAAA|\n|AAAAAAAAACAFAAAA|\n|AAAAAAAAACAFAAAA|\n|AAAAAAAAACAHBAAA|\n|AAAAAAAAACAJAAAA|\n|AAAAAAAAACBDAAAA|\n|AAAAAAAAACBDAAAA|\n|AAAAAAAAACBEBAAA|\n|AAAAAAAAACBNAAAA|\n|AAAAAAAAACBPAAAA|\n|AAAAAAAAACCHAAAA|\n+----------------+\n\n", 
                    "output_type": "stream", 
                    "name": "stdout"
                }
            ], 
            "metadata": {}, 
            "source": "val QUERY_NUM = 1\nval result = runIndividualQuery(QUERY_NUM)\ndisplaySummary(result)\ndisplayResult(QUERY_NUM, result)", 
            "execution_count": 8
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "## Run all the TPC-DS queries\n* Runs all the queries starting from 1 to 99\n* The query results are saved and can be queried by calling getResults method.\n* The summary will be shown at the end."
        }, 
        {
            "cell_type": "code", 
            "outputs": [
                {
                    "text": "Running TPC-DS Query : 01\nsingle query path\nRunning TPC-DS Query : 02\nsingle query path\nRunning TPC-DS Query : 03\nsingle query path\nRunning TPC-DS Query : 04\nsingle query path\nRunning TPC-DS Query : 05\nsingle query path\nRunning TPC-DS Query : 06\nsingle query path\nRunning TPC-DS Query : 07\nsingle query path\nRunning TPC-DS Query : 08\nsingle query path\nRunning TPC-DS Query : 09\nsingle query path\nRunning TPC-DS Query : 10\nsingle query path\nRunning TPC-DS Query : 11\nsingle query path\nRunning TPC-DS Query : 12\nsingle query path\nRunning TPC-DS Query : 13\nsingle query path\nRunning TPC-DS Query : 14\nmulti query path =1\nmulti query path =2\nRunning TPC-DS Query : 15\nsingle query path\nRunning TPC-DS Query : 16\nsingle query path\nRunning TPC-DS Query : 17\nsingle query path\nRunning TPC-DS Query : 18\nsingle query path\nRunning TPC-DS Query : 19\nsingle query path\nRunning TPC-DS Query : 20\nsingle query path\nRunning TPC-DS Query : 21\nsingle query path\nRunning TPC-DS Query : 22\nsingle query path\nRunning TPC-DS Query : 23\nmulti query path =1\nmulti query path =2\nRunning TPC-DS Query : 24\nmulti query path =1\nmulti query path =2\nRunning TPC-DS Query : 25\nsingle query path\nRunning TPC-DS Query : 26\nsingle query path\nRunning TPC-DS Query : 27\nsingle query path\nRunning TPC-DS Query : 28\nsingle query path\nRunning TPC-DS Query : 29\nsingle query path\nRunning TPC-DS Query : 30\nsingle query path\nRunning TPC-DS Query : 31\nsingle query path\nRunning TPC-DS Query : 32\nsingle query path\nRunning TPC-DS Query : 33\nsingle query path\nRunning TPC-DS Query : 34\nsingle query path\nRunning TPC-DS Query : 35\nsingle query path\nRunning TPC-DS Query : 36\nsingle query path\nRunning TPC-DS Query : 37\nsingle query path\nRunning TPC-DS Query : 38\nsingle query path\nRunning TPC-DS Query : 39\nmulti query path =1\nmulti query path =2\nRunning TPC-DS Query : 40\nsingle query path\nRunning TPC-DS Query : 41\nsingle query path\nRunning TPC-DS Query : 42\nsingle query path\nRunning TPC-DS Query : 43\nsingle query path\nRunning TPC-DS Query : 44\nsingle query path\nRunning TPC-DS Query : 45\nsingle query path\nRunning TPC-DS Query : 46\nsingle query path\nRunning TPC-DS Query : 47\nsingle query path\nRunning TPC-DS Query : 48\nsingle query path\nRunning TPC-DS Query : 49\nsingle query path\nRunning TPC-DS Query : 50\nsingle query path\nRunning TPC-DS Query : 51\nsingle query path\nRunning TPC-DS Query : 52\nsingle query path\nRunning TPC-DS Query : 53\nsingle query path\nRunning TPC-DS Query : 54\nsingle query path\nRunning TPC-DS Query : 55\nsingle query path\nRunning TPC-DS Query : 56\nsingle query path\nRunning TPC-DS Query : 57\nsingle query path\nRunning TPC-DS Query : 58\nsingle query path\nRunning TPC-DS Query : 59\nsingle query path\nRunning TPC-DS Query : 60\nsingle query path\nRunning TPC-DS Query : 61\nsingle query path\nRunning TPC-DS Query : 62\nsingle query path\nRunning TPC-DS Query : 63\nsingle query path\nRunning TPC-DS Query : 64\nsingle query path\nRunning TPC-DS Query : 65\nsingle query path\nRunning TPC-DS Query : 66\nsingle query path\nRunning TPC-DS Query : 67\n", 
                    "output_type": "stream", 
                    "name": "stdout"
                }
            ], 
            "metadata": {}, 
            "source": "val result = runAllQueries()\ndisplaySummary(result)", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "## Display Result for a individual Query\n* Reads the result file for the given query stored when thery are run in previous steps.\n* Certain queries have multiple associated result files. The result files are read in sequence and\n  results are displayed.\n* If the result file(s) are not found , then an error is displayed.  \n"
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {}, 
            "source": "displayResult(14, result)", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "## Display SQL Execution Plan\n* Display the analyzed, optimized and phyical plan for a given query.\n* Can be used by developers for debugging purposes.\n* QUERY_NUM can be changed to display the plan for different query."
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {}, 
            "source": "explainQuery(1)", 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "outputs": [], 
            "metadata": {}, 
            "source": "%%brunel\ndata('result') bar x(QueryName) y(ElapsedTime) title(\"Query Execution Time in seconds\", \"Execution Summary\":footer)", 
            "execution_count": null
        }
    ], 
    "nbformat": 4, 
    "metadata": {
        "kernelspec": {
            "display_name": "Scala 2.11 with Spark 2.1", 
            "name": "scala-spark21", 
            "language": "scala"
        }, 
        "language_info": {
            "version": "2.11.8", 
            "mimetype": "text/x-scala", 
            "codemirror_mode": "text/x-scala", 
            "file_extension": ".scala", 
            "name": "scala", 
            "pygments_lexer": "scala"
        }
    }, 
    "nbformat_minor": 1
}