{
    "metadata": {
        "language_info": {
            "name": "scala", 
            "mimetype": "text/x-scala", 
            "pygments_lexer": "scala", 
            "codemirror_mode": "text/x-scala", 
            "file_extension": ".scala", 
            "version": "2.11.8"
        }, 
        "kernelspec": {
            "display_name": "Scala 2.11 with Spark 2.1", 
            "name": "scala-spark21", 
            "language": "scala"
        }
    }, 
    "nbformat": 4, 
    "cells": [
        {
            "metadata": {}, 
            "source": "## Get the journey data.\nClones the tpcds journey repository to get access to all the data and scripts that are required to excercise this journey. Normally the data and queries are generated by running the data and query generation utility from the tpcds toolkit available at http://www.tpc.org/tpcds. However for easy of use, the data and queries are pre-generated for 1GB scale factor. We use the pre-generated data and queries to demonstrate how they can be used to run the tpcds queries against spark.\n", 
            "cell_type": "markdown"
        }, 
        {
            "source": "import sys.process._\n\"rm -rf tpcds-journey\" !\n\"git --version\" !\n\"git clone --progress https://github.com/SparkTC/tpcds-journey.git\" !", 
            "metadata": {}, 
            "execution_count": 1, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "git version 1.8.3.1\nCloning into 'tpcds-journey'...\nremote: Counting objects: 991, done.        \nremote: Compressing objects: 100% (106/106), done.        % (102/106)           \nremote: Total 991 (delta 87), reused 97 (delta 41), pack-reused 843        \nReceiving objects: 100% (991/991), 362.47 MiB | 16.63 MiB/s, done.\nResolving deltas: 100% (396/396), done.\nChecking out files: 100% (806/806), done.\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "## Setup variables.\n* Sets up variables that are used in the rest of this notebook.\n* The path variables are relative to the git clone directory.\n* tpcdsDatabaseName is hard-coded to \"TPCDS1G\". This can be changed if a different database name is desired.", 
            "cell_type": "markdown"
        }, 
        {
            "source": "def deleteFile1(tableName: String): Unit = {\n    import sys.process._\n    val commandStr1 = s\"rm -rf spark-warehouse/tpcds2g.db/${tableName}/*\"\n    val commandStr2 = s\"rm -rf spark-warehouse/tpcds2g.db/${tableName}\"\n    commandStr1 !\n}", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 2, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "def deleteFile2(tableName: String): Unit = {\n    import sys.process._\n    val commandStr2 = s\"rm -rf spark-warehouse/tpcds2g.db/${tableName}\"\n    commandStr2 !\n}", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 3, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "val tpcdsRootDir = \"tpcds-journey\"\nval tpcdsDdlDir = s\"${tpcdsRootDir}/src/ddl/individual\"\nval tpcdsGenDataDir = s\"${tpcdsRootDir}/src/data\"\nval tpcdsQueriesDir = s\"${tpcdsRootDir}/src/queries\"\nval tpcdsDatabaseName = \"TPCDS2G\"\nvar totalTime: Long = 0\nprintln(\"TPCDS root directory is at : \"+ tpcdsRootDir)\nprintln(\"TPCDS ddl scripts directory is at: \" + tpcdsDdlDir)\nprintln(\"TPCDS data directory is at: \"+ tpcdsGenDataDir)\nprintln(\"TPCDS queries directory is at: \"+ tpcdsQueriesDir)\n\n// val conf = sc.getConf\n// println(conf.toDebugString)\nval journey_spark = SparkSession.\n    builder().\n    config(\"spark.ui.showConsoleProgress\", false).\n    config(\"spark.ui.enabled\", false).\n    config(\"spark.ui.retainedJobs\", 1).\n    config(\"spark.ui.retainedStages\", 1).\n    config(\"spark.ui.retainedTasks\", 1).\n    config(\"spark.sql.autoBroadcastJoinThreshold\", -1).\n    config(\"spark.sql.crossJoin.enabled\", true).\n    config(\"spark.storage.memoryFraction\", \"0.1\").\n    config(\"spark.dynamicAllocation.enabled\", \"true\").\n    config(\"spark.executor.instances\", 4).\n    config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\").\n    getOrCreate()\n\njourney_spark.sparkContext.setLogLevel(\"ERROR\")", 
            "metadata": {}, 
            "execution_count": 4, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "TPCDS root directory is at : tpcds-journey\nTPCDS ddl scripts directory is at: tpcds-journey/src/ddl/individual\nTPCDS data directory is at: tpcds-journey/src/data\nTPCDS queries directory is at: tpcds-journey/src/queries\n"
                }
            ]
        }, 
        {
            "source": "", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "## Setup the TPC-DS schema\n* Creates the database as specified by tpcdsDatabaseName\n* Creates all the tpc-ds tables.\n* Loads data into the tables in parquet format. \n  * Since the data generated by tpc-ds toolkit is in CSV format, we do the loading in multi steps.\n  * As first step, we create tables in csv format by pointing the location to the generated data.\n  * As second step, we create parquet tables by using CTAS that convert text data into parquet.\n  * As last step, we drop the text tables as we longer need them.", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "### Utility function definitions.\n* Defines the utility functions that are called from the cells below in the notebook.", 
            "cell_type": "markdown"
        }, 
        {
            "source": "def createDatabase(): Unit = {\n    journey_spark.sql(s\"DROP DATABASE IF EXISTS ${tpcdsDatabaseName} CASCADE\")\n    journey_spark.sql(s\"CREATE DATABASE ${tpcdsDatabaseName}\")\n    journey_spark.sql(s\"USE ${tpcdsDatabaseName}\")\n}\n\n/**\n * Function to create a table in spark. It reads the DDL script for each of the\n * tpc-ds table and executes it on Spark.\n */\ndef createTable(tableName: String): Unit = {\n  println(s\"Creating table $tableName ..\")\n  journey_spark.sql(s\"DROP TABLE IF EXISTS $tableName\")\n  deleteFile1(tableName)   \n  deleteFile2(tableName)    \n  val (fileName, content) = \n    journey_spark.sparkContext.wholeTextFiles(s\"${tpcdsDdlDir}/$tableName.sql\").collect()(0) \n    \n  // Remove the replace for the .dat once it is fixed in the github repo\n  val sqlStmts = content.stripLineEnd\n    .replace('\\n', ' ')\n    .replace(\"${TPCDS_GENDATA_DIR}\", tpcdsGenDataDir)\n    .replace(\"csv\", \"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\").split(\";\")\n  sqlStmts.map(stmt => journey_spark.sql(stmt))    \n}  \n\ndef runQuery(queryNum: Int): Unit = {\n  val queryStr = \"%02d\".format(queryNum)  \n  val queryName = s\"${tpcdsQueriesDir}/query${queryStr}.sql\"   \n  val (_, content) = journey_spark.sparkContext.wholeTextFiles(queryName).collect()(0)  \n  val queries = content.split(\"\\n\")\n    .filterNot (_.startsWith(\"--\"))\n    .mkString(\" \").split(\";\")\n  println(s\"Running TPC-DS Query $queryName\")\n  for (query <- queries)  {\n   val start = System.nanoTime()\n   val result = journey_spark.sql(query).collect  \n   val timeElapsed = (System.nanoTime() - start) / 1000000000\n   totalTime = totalTime + timeElapsed\n   // result.foreach(println) \n   println(s\"Time elapsed: $timeElapsed, Number of rows: ${result.length}\") \n  }\n}\n\ndef runAllQueries(): Unit = {\n    \n}\n\n// run function for each table in tables array\ndef forEachTable(tables: Array[String], f: (String) => Unit): Unit = {\n  for ( table <- tables) {\n    try {\n      f(table)\n    } catch {\n      case e: Throwable => {\n        println(\"EXCEPTION!! \" + e.getMessage())\n        throw e\n      }\n    }\n  }\n}", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 5, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "### Create the database and tables.\n* Creates the tpc-ds database.\n* For each of the table name in TPC-DS schema, calls up on the function to create the table in spark.", 
            "cell_type": "markdown"
        }, 
        {
            "source": "// TPC-DS table names.\nval tables = Array(\"call_center\", \"catalog_sales\",\n                   \"customer_demographics\", \"income_band\",\n                   \"promotion\", \"store\", \"time_dim\", \"web_returns\",\n                   \"catalog_page\", \"customer\", \"date_dim\",\n                   \"inventory\", \"reason\", \"store_returns\", \"warehouse\",\n                   \"web_sales\", \"catalog_returns\", \"customer_address\",\n                   \"household_demographics\", \"item\", \"ship_mode\", \"store_sales\",\n                   \"web_page\", \"web_site\" )\n\n// Create database\ncreateDatabase\n\n// Create table\nforEachTable(tables, table => createTable(table))\n", 
            "metadata": {}, 
            "execution_count": 6, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Creating table call_center ..\nCreating table catalog_sales ..\nCreating table customer_demographics ..\nCreating table income_band ..\nCreating table promotion ..\nCreating table store ..\nCreating table time_dim ..\nCreating table web_returns ..\nCreating table catalog_page ..\nCreating table customer ..\nCreating table date_dim ..\nCreating table inventory ..\nCreating table reason ..\nCreating table store_returns ..\nCreating table warehouse ..\nCreating table web_sales ..\nCreating table catalog_returns ..\nCreating table customer_address ..\nCreating table household_demographics ..\nCreating table item ..\nCreating table ship_mode ..\nCreating table store_sales ..\nCreating table web_page ..\nCreating table web_site ..\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "## Verify table creating and data loading.\n* Run a simple Spark SQL query to get the count of rows\n* Verify that the row counts are as expected", 
            "cell_type": "markdown"
        }, 
        {
            "source": "// Run a count query and get the counts\nval rowCounts = tables.map { table =>\n    journey_spark.table(table).count()\n}\n\nval expectedCounts = Array (\n    6, 1441548, 1920800, 20, 300, 12, 86400,\n    71763,  11718, 100000, 73049, 11745000, \n    35, 287514, 5, 719384, 144067, 50000, 7200,\n    18000, 20, 2880404, 60, 30\n)\n\nvar errorCount = 0;\nval zippedCountsWithIndex = rowCounts.zip(expectedCounts).zipWithIndex\nfor ((pair, index) <- zippedCountsWithIndex) {\n    if (pair._1 != pair._2) {\n        println(s\"\"\"ERROR!! Row counts for ${tables(index)} does not match.\n        Expected=${expectedCounts(index)} but found ${rowCounts(index)}\"\"\")\n        errorCount += 1\n    }\n}\n\nprintln(\"=====================================================\")\nif ( errorCount > 0) {\n  println(s\"Load verification failed with $errorCount errors\")\n} else {\n  println(\"Loaded and verified the table counts successfully\")\n}\nprintln(\"=====================================================\")", 
            "metadata": {}, 
            "execution_count": 7, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "=====================================================\nLoaded and verified the table counts successfully\n=====================================================\n"
                }
            ]
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "## Run a query by its number\n* Given a query number between 1 to 99, run it against spark.\n* Display the query results, time taken to execute the query and number of rows returned.\n* To run a differnt query , please change the QUERY_NUM to a valid value from 1 to 99.", 
            "cell_type": "markdown"
        }, 
        {
            "source": "val QUERY_NUM = 32\nrunQuery(QUERY_NUM)", 
            "metadata": {}, 
            "execution_count": 8, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Running TPC-DS Query tpcds-journey/src/queries/query32.sql\nTime elapsed: 11, Number of rows: 1\n"
                }
            ]
        }, 
        {
            "source": "var queryErrors = 0\nfor (i <- 1 to 33) {\n  try{\n    runQuery(i)\n  } catch {\n     case e: Throwable => {\n          println(\"Error in query \"+ i + \"msg = \" + e.getMessage)\n          queryErrors += 1\n     }\n  }\n}\nprintln(\"=====================================================\")\nif ( queryErrors > 0) {\n  println(s\"Query execution failed with $queryErrors errors\")\n} else {\n  println(\"All TPC-DS queries ran successfully\")\n}\nprintln (s\"Total Elapsed Time so far: ${totalTime} seconds.\")\nprintln(\"=====================================================\")", 
            "metadata": {}, 
            "execution_count": 9, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Running TPC-DS Query tpcds-journey/src/queries/query01.sql\nTime elapsed: 14, Number of rows: 100\nRunning TPC-DS Query tpcds-journey/src/queries/query02.sql\nTime elapsed: 6, Number of rows: 2513\nRunning TPC-DS Query tpcds-journey/src/queries/query03.sql\nTime elapsed: 11, Number of rows: 89\nRunning TPC-DS Query tpcds-journey/src/queries/query04.sql\nTime elapsed: 41, Number of rows: 8\nRunning TPC-DS Query tpcds-journey/src/queries/query05.sql\nTime elapsed: 18, Number of rows: 100\nRunning TPC-DS Query tpcds-journey/src/queries/query06.sql\nTime elapsed: 26, Number of rows: 45\nRunning TPC-DS Query tpcds-journey/src/queries/query07.sql\nTime elapsed: 8, Number of rows: 100\nRunning TPC-DS Query tpcds-journey/src/queries/query08.sql\nTime elapsed: 9, Number of rows: 5\nRunning TPC-DS Query tpcds-journey/src/queries/query09.sql\nTime elapsed: 2, Number of rows: 1\nRunning TPC-DS Query tpcds-journey/src/queries/query10.sql\nTime elapsed: 10, Number of rows: 5\nRunning TPC-DS Query tpcds-journey/src/queries/query11.sql\nTime elapsed: 36, Number of rows: 88\nRunning TPC-DS Query tpcds-journey/src/queries/query12.sql\nTime elapsed: 8, Number of rows: 100\nRunning TPC-DS Query tpcds-journey/src/queries/query13.sql\nTime elapsed: 16, Number of rows: 1\nRunning TPC-DS Query tpcds-journey/src/queries/query14.sql\nTime elapsed: 81, Number of rows: 100\nTime elapsed: 49, Number of rows: 100\nRunning TPC-DS Query tpcds-journey/src/queries/query15.sql\nTime elapsed: 14, Number of rows: 100\nRunning TPC-DS Query tpcds-journey/src/queries/query16.sql\nTime elapsed: 12, Number of rows: 1\nRunning TPC-DS Query tpcds-journey/src/queries/query17.sql\nTime elapsed: 13, Number of rows: 1\nTime elapsed: 11, Number of rows: 100\nRunning TPC-DS Query tpcds-journey/src/queries/query19.sql\nTime elapsed: 14, Number of rows: 100\nRunning TPC-DS Query tpcds-journey/src/queries/query20.sql\nTime elapsed: 15, Number of rows: 100\nRunning TPC-DS Query tpcds-journey/src/queries/query21.sql\nTime elapsed: 12, Number of rows: 100\nRunning TPC-DS Query tpcds-journey/src/queries/query22.sql\nTime elapsed: 20, Number of rows: 100\nRunning TPC-DS Query tpcds-journey/src/queries/query23.sql\nTime elapsed: 19, Number of rows: 1\nTime elapsed: 32, Number of rows: 4\nRunning TPC-DS Query tpcds-journey/src/queries/query24.sql\nTime elapsed: 30, Number of rows: 0\nTime elapsed: 20, Number of rows: 0\nRunning TPC-DS Query tpcds-journey/src/queries/query25.sql\nTime elapsed: 13, Number of rows: 1\nRunning TPC-DS Query tpcds-journey/src/queries/query26.sql\nTime elapsed: 5, Number of rows: 100\nRunning TPC-DS Query tpcds-journey/src/queries/query27.sql\nTime elapsed: 5, Number of rows: 100\nRunning TPC-DS Query tpcds-journey/src/queries/query28.sql\nTime elapsed: 4, Number of rows: 1\nRunning TPC-DS Query tpcds-journey/src/queries/query29.sql\nTime elapsed: 12, Number of rows: 1\nRunning TPC-DS Query tpcds-journey/src/queries/query30.sql\nTime elapsed: 16, Number of rows: 100\nRunning TPC-DS Query tpcds-journey/src/queries/query31.sql\nTime elapsed: 16, Number of rows: 51\nRunning TPC-DS Query tpcds-journey/src/queries/query32.sql\nTime elapsed: 5, Number of rows: 1\nRunning TPC-DS Query tpcds-journey/src/queries/query33.sql\nTime elapsed: 14, Number of rows: 100\n=====================================================\nAll TPC-DS queries ran successfully\nTotal Elapsed Time so far: 648 seconds.\n=====================================================\n"
                }
            ]
        }, 
        {
            "source": "var queryErrors = 0\nfor (i <- 34 to 66) {\n  try{\n    runQuery(i)\n  } catch {\n     case e: Throwable => {\n          println(\"Error in query \"+ i )\n          queryErrors += 1\n     }\n  }\n}\nprintln(\"=====================================================\")\nif ( queryErrors > 0) {\n  println(s\"Query execution failed with $queryErrors errors\")\n} else {\n  println(\"All TPC-DS queries ran successfully\")\n}\nprintln (s\"Total Elapsed Time so far: ${totalTime} seconds.\")\nprintln(\"=====================================================\")", 
            "metadata": {}, 
            "execution_count": 10, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Running TPC-DS Query tpcds-journey/src/queries/query34.sql\nTime elapsed: 7, Number of rows: 451\nRunning TPC-DS Query tpcds-journey/src/queries/query35.sql\nTime elapsed: 15, Number of rows: 100\nRunning TPC-DS Query tpcds-journey/src/queries/query36.sql\nTime elapsed: 7, Number of rows: 100\nRunning TPC-DS Query tpcds-journey/src/queries/query37.sql\nTime elapsed: 2, Number of rows: 1\nRunning TPC-DS Query tpcds-journey/src/queries/query38.sql\nTime elapsed: 17, Number of rows: 1\nRunning TPC-DS Query tpcds-journey/src/queries/query39.sql\nTime elapsed: 25, Number of rows: 246\nTime elapsed: 31, Number of rows: 17\nRunning TPC-DS Query tpcds-journey/src/queries/query40.sql\nTime elapsed: 7, Number of rows: 100\nRunning TPC-DS Query tpcds-journey/src/queries/query41.sql\nTime elapsed: 1, Number of rows: 4\nRunning TPC-DS Query tpcds-journey/src/queries/query42.sql\nTime elapsed: 3, Number of rows: 10\nRunning TPC-DS Query tpcds-journey/src/queries/query43.sql\nTime elapsed: 3, Number of rows: 6\nRunning TPC-DS Query tpcds-journey/src/queries/query44.sql\nTime elapsed: 4, Number of rows: 10\nRunning TPC-DS Query tpcds-journey/src/queries/query45.sql\nTime elapsed: 8, Number of rows: 19\nRunning TPC-DS Query tpcds-journey/src/queries/query46.sql\nTime elapsed: 14, Number of rows: 100\nRunning TPC-DS Query tpcds-journey/src/queries/query47.sql\nTime elapsed: 42, Number of rows: 100\nRunning TPC-DS Query tpcds-journey/src/queries/query48.sql\nTime elapsed: 7, Number of rows: 1\nRunning TPC-DS Query tpcds-journey/src/queries/query49.sql\nTime elapsed: 5, Number of rows: 32\nRunning TPC-DS Query tpcds-journey/src/queries/query50.sql\nTime elapsed: 11, Number of rows: 6\nRunning TPC-DS Query tpcds-journey/src/queries/query51.sql\nTime elapsed: 29, Number of rows: 100\nRunning TPC-DS Query tpcds-journey/src/queries/query52.sql\nTime elapsed: 7, Number of rows: 100\nRunning TPC-DS Query tpcds-journey/src/queries/query53.sql\nTime elapsed: 7, Number of rows: 100\nRunning TPC-DS Query tpcds-journey/src/queries/query54.sql\nTime elapsed: 12, Number of rows: 1\nRunning TPC-DS Query tpcds-journey/src/queries/query55.sql\nTime elapsed: 8, Number of rows: 100\nRunning TPC-DS Query tpcds-journey/src/queries/query56.sql\nTime elapsed: 8, Number of rows: 100\nRunning TPC-DS Query tpcds-journey/src/queries/query57.sql\nTime elapsed: 39, Number of rows: 100\nRunning TPC-DS Query tpcds-journey/src/queries/query58.sql\nTime elapsed: 15, Number of rows: 3\nRunning TPC-DS Query tpcds-journey/src/queries/query59.sql\nTime elapsed: 5, Number of rows: 100\nRunning TPC-DS Query tpcds-journey/src/queries/query60.sql\nTime elapsed: 9, Number of rows: 100\nRunning TPC-DS Query tpcds-journey/src/queries/query61.sql\nTime elapsed: 13, Number of rows: 1\nRunning TPC-DS Query tpcds-journey/src/queries/query62.sql\nTime elapsed: 5, Number of rows: 100\nRunning TPC-DS Query tpcds-journey/src/queries/query63.sql\nTime elapsed: 9, Number of rows: 100\nRunning TPC-DS Query tpcds-journey/src/queries/query64.sql\nTime elapsed: 139, Number of rows: 10\nRunning TPC-DS Query tpcds-journey/src/queries/query65.sql\nTime elapsed: 11, Number of rows: 100\nRunning TPC-DS Query tpcds-journey/src/queries/query66.sql\nTime elapsed: 8, Number of rows: 5\n=====================================================\nAll TPC-DS queries ran successfully\nTotal Elapsed Time so far: 1181 seconds.\n=====================================================\n"
                }
            ]
        }, 
        {
            "source": "var queryErrors = 0\nfor (i <- 67 to 99) {\n  try{\n    runQuery(i)\n  } catch {\n     case e: Throwable => {\n          println(\"Error in query \"+ i )\n          queryErrors += 1\n     }\n  }\n}\nprintln(\"=====================================================\")\nif ( queryErrors > 0) {\n  println(s\"Query execution failed with $queryErrors errors\")\n} else {\n  println(\"All TPC-DS queries ran successfully\")\n}\nprintln (s\"Total Elapsed Time so far: ${totalTime} seconds.\")\nprintln(\"=====================================================\")", 
            "metadata": {}, 
            "execution_count": 11, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Running TPC-DS Query tpcds-journey/src/queries/query67.sql\nTime elapsed: 11, Number of rows: 100\nRunning TPC-DS Query tpcds-journey/src/queries/query68.sql\nTime elapsed: 7, Number of rows: 100\nRunning TPC-DS Query tpcds-journey/src/queries/query69.sql\nTime elapsed: 12, Number of rows: 100\nRunning TPC-DS Query tpcds-journey/src/queries/query70.sql\nTime elapsed: 6, Number of rows: 3\nRunning TPC-DS Query tpcds-journey/src/queries/query71.sql\nTime elapsed: 9, Number of rows: 1018\nRunning TPC-DS Query tpcds-journey/src/queries/query72.sql\nTime elapsed: 101, Number of rows: 100\nRunning TPC-DS Query tpcds-journey/src/queries/query73.sql\nTime elapsed: 5, Number of rows: 5\nRunning TPC-DS Query tpcds-journey/src/queries/query74.sql\nTime elapsed: 26, Number of rows: 92\nRunning TPC-DS Query tpcds-journey/src/queries/query75.sql\nTime elapsed: 52, Number of rows: 100\nRunning TPC-DS Query tpcds-journey/src/queries/query76.sql\nTime elapsed: 9, Number of rows: 100\nRunning TPC-DS Query tpcds-journey/src/queries/query77.sql\nException in thread \"dispatcher-event-loop-45\" java.lang.OutOfMemoryError: Java heap space\nonfun$receive$1.applyOrElse(EGOFineGrainedSchedulerBackend.scala:786)\n\tat org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)\n\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)\n\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n\tat org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1153)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.lang.Thread.run(Thread.java:785)\n\tat sun.reflect.UTF8.encode(UTF8.java:48)\n\tat sun.reflect.ClassFileAssembler.emitConstantPoolUTF8(ClassFileAssembler.java:115)\n\tat sun.reflect.MethodAccessorGenerator.generate(MethodAccessorGenerator.java:343)\n\tat sun.reflect.MethodAccessorGenerator.generateSerializationConstructor(MethodAccessorGenerator.java:124)\n\tat sun.reflect.ReflectionFactory.newConstructorForSerialization(ReflectionFactory.java:352)\n\tat java.io.ObjectStreamClass.getSerializableConstructor(ObjectStreamClass.java:1512)\n\tat java.io.ObjectStreamClass.access$1500(ObjectStreamClass.java:85)\n\tat java.io.ObjectStreamClass$3.run(ObjectStreamClass.java:578)\n\tat java.io.ObjectStreamClass$3.run(ObjectStreamClass.java:553)\n\tat java.security.AccessController.doPrivileged(AccessController.java:594)\n\tat java.io.ObjectStreamClass.<init>(ObjectStreamClass.java:553)\n\tat java.io.ObjectStreamClass.lookup(ObjectStreamClass.java:397)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1164)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1633)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1594)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1517)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1217)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1633)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1594)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1517)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1217)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:368)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)\n\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)\n\tat org.apache.spark.scheduler.Task$.serializeWithDependencies(Task.scala:246)\n\tat org.apache.spark.scheduler.TaskSetManager$$anonfun$resourceOffer$1.apply(TaskSetManager.scala:452)\n\tat org.apache.spark.scheduler.TaskSetManager$$anonfun$resourceOffer$1.apply(TaskSetManager.scala:432)\n\tat scala.Option.map(Option.scala:146)\n\tat org.apache.spark.scheduler.TaskSetManager.resourceOffer(TaskSetManager.scala:432)\n\tat org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$org$apache$spark$scheduler$TaskSchedulerImpl$$resourceOfferSingleTaskSet$1.apply$mcVI$sp(TaskSchedulerImpl.scala:271)\n\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)\n\tat org.apache.spark.scheduler.TaskSchedulerImpl.org$apache$spark$scheduler$TaskSchedulerImpl$$resourceOfferSingleTaskSet(TaskSchedulerImpl.scala:266)\n\tat org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$resourceOffers$3$$anonfun$apply$8.apply(TaskSchedulerImpl.scala:343)\n\tat org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$resourceOffers$3$$anonfun$apply$8.apply(TaskSchedulerImpl.scala:338)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$resourceOffers$3.apply(TaskSchedulerImpl.scala:338)\n\tat org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$resourceOffers$3.apply(TaskSchedulerImpl.scala:335)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOffers(TaskSchedulerImpl.scala:335)\n\tat org.apache.spark.scheduler.cluster.ego.EGOFineGrainedSchedulerBackend$EGODriverEndpoint.makeOffers(EGOFineGrainedSchedulerBackend.scala:1004)\n\tat org.apache.spark.scheduler.cluster.ego.EGOFineGrainedSchedulerBackend$EGODriverEndpoint$$anException in thread \"Spark Context Cleaner\" java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1.apply$mcV$sp(ContextCleaner.scala:184)\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1358)\n\tat org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:180)\n\tat org.apache.spark.ContextCleaner$$anon$1.run(ContextCleaner.scala:67)\nError in query 77\nError in query 78\nError in query 79\nError in query 80\nError in query 81\nError in query 82\nError in query 83\nError in query 84\nError in query 85\nError in query 86\nError in query 87\nError in query 88\nError in query 89\nError in query 90\nError in query 91\nError in query 92\nError in query 93\nError in query 94\nError in query 95\nError in query 96\nError in query 97\nError in query 98\nError in query 99\n=====================================================\nQuery execution failed with 23 errors\nTotal Elapsed Time so far: 1419 seconds.\n=====================================================\n"
                }
            ]
        }, 
        {
            "source": "", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }
    ], 
    "nbformat_minor": 1
}