{
    "cells": [
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "## Get the journey data.\nClones the tpcds journey repository to get access to all the data and scripts that are required to excercise this journey. Normally the data and queries are generated by running the data and query generation utility from the tpcds toolkit available at http://www.tpc.org/tpcds. However for easy of use, the data and queries are pre-generated for 1GB scale factor. We use the pre-generated data and queries to demonstrate how they can be used to run the tpcds queries against spark.\n"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 1, 
            "outputs": [], 
            "source": "import sys.process._"
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "execution_count": 2, 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "Found 1 items\n-rw-r--r--   3 clsadmin biusers       1885 2017-10-05 00:20 /user/clsadmin/tpcds-journey/src/data/call_center/call_center.dat\n", 
                    "output_type": "stream"
                }, 
                {
                    "metadata": {}, 
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "warning: there was one feature warning; re-run with -feature for details\n"
                    }
                }, 
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "0"
                    }, 
                    "execution_count": 2
                }
            ], 
            "source": "\"hdfs dfs -ls /user/clsadmin/tpcds-journey/src/data/call_center\" !"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "## Setup variables.\n* Sets up variables that are used in the rest of this notebook.\n* The path variables are relative to the git clone directory.\n* tpcdsDatabaseName is hard-coded to \"TPCDS1G\". This can be changed if a different database name is desired."
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "execution_count": 3, 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "TPCDS root directory is at : /user/clsadmin/tpcds-journey\nTPCDS ddl scripts directory is at: /user/clsadmin/tpcds-journey/src/ddl/individual\nTPCDS data directory is at: /user/clsadmin/tpcds-journey/src/data\nTPCDS queries directory is at: /user/clsadmin/tpcds-journey/src/queries\n", 
                    "output_type": "stream"
                }, 
                {
                    "metadata": {}, 
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "tpcdsRootDir = /user/clsadmin/tpcds-journey\ntpcdsDdlDir = /user/clsadmin/tpcds-journey/src/ddl/individual\ntpcdsGenDataDir = /user/clsadmin/tpcds-journey/src/data\ntpcdsQueriesDir = /user/clsadmin/tpcds-journey/src/queries\ntpcdsDatabaseName = TPCDS2G\ntotalTime = 0\njourney_spark = org.apache.spark.sql.SparkSession@54816ed3\n"
                    }
                }, 
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/html": "<ul>\n<li><a href=\"None\" target=\"new_tab\">Spark UI: application_1507177189916_0003</a></li>\n</ul>", 
                        "text/plain": "Spark application_1507177189916_0003: None"
                    }, 
                    "execution_count": 3
                }
            ], 
            "source": "val tpcdsRootDir = \"/user/clsadmin/tpcds-journey\"\nval tpcdsDdlDir = s\"${tpcdsRootDir}/src/ddl/individual\"\nval tpcdsGenDataDir = s\"${tpcdsRootDir}/src/data\"\nval tpcdsQueriesDir = s\"${tpcdsRootDir}/src/queries\"\nval tpcdsDatabaseName = \"TPCDS2G\"\nvar totalTime: Long = 0\nprintln(\"TPCDS root directory is at : \"+ tpcdsRootDir)\nprintln(\"TPCDS ddl scripts directory is at: \" + tpcdsDdlDir)\nprintln(\"TPCDS data directory is at: \"+ tpcdsGenDataDir)\nprintln(\"TPCDS queries directory is at: \"+ tpcdsQueriesDir)\n\n// val conf = sc.getConf\n// println(conf.toDebugString)\nval journey_spark = SparkSession.\n    builder().\n    config(\"spark.ui.showConsoleProgress\", false).\n    config(\"spark.ui.enabled\", false).\n    config(\"spark.ui.retainedJobs\", 1).\n    config(\"spark.ui.retainedStages\", 1).\n    config(\"spark.ui.retainedTasks\", 1).\n    config(\"spark.sql.autoBroadcastJoinThreshold\", -1).\n    config(\"spark.sql.crossJoin.enabled\", true).\n    config(\"spark.storage.memoryFraction\", \"0.1\").\n    config(\"spark.shuffle.service.enabled\", true).\n    config(\"spark.dynamicAllocation.enabled\", \"true\").\n    config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\").\n    getOrCreate()"
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "execution_count": 4, 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "+--------+---------+-----------+\n|database|tableName|isTemporary|\n+--------+---------+-----------+\n|default |t2       |false      |\n+--------+---------+-----------+\n\n", 
                    "output_type": "stream"
                }
            ], 
            "source": "journey_spark.sql(\"show tables\").show(false)"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "## Setup the TPC-DS schema\n* Creates the database as specified by tpcdsDatabaseName\n* Creates all the tpc-ds tables.\n* Loads data into the tables in parquet format. \n  * Since the data generated by tpc-ds toolkit is in CSV format, we do the loading in multi steps.\n  * As first step, we create tables in csv format by pointing the location to the generated data.\n  * As second step, we create parquet tables by using CTAS that convert text data into parquet.\n  * As last step, we drop the text tables as we longer need them."
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### Utility function definitions.\n* Defines the utility functions that are called from the cells below in the notebook."
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "execution_count": 5, 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "createDatabase: ()Unit\n"
                    }
                }
            ], 
            "source": "def createDatabase(): Unit = {\n    journey_spark.sql(s\"DROP DATABASE IF EXISTS ${tpcdsDatabaseName} CASCADE\")\n    journey_spark.sql(s\"CREATE DATABASE ${tpcdsDatabaseName}\")\n    journey_spark.sql(s\"USE ${tpcdsDatabaseName}\")\n    println(s\"Finish creating database ${tpcdsDatabaseName}\")\n}"
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "execution_count": 6, 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "createTable: (tableName: String)Unit\n"
                    }
                }
            ], 
            "source": "/**\n * Function to create a table in spark. It reads the DDL script for each of the\n * tpc-ds table and executes it on Spark.\n */\ndef createTable(tableName: String): Unit = {\n  println(s\"Creating table $tableName ..\")\n  journey_spark.sql(s\"DROP TABLE IF EXISTS $tableName\")   \n  val (fileName, content) = \n    journey_spark.sparkContext.wholeTextFiles(s\"${tpcdsDdlDir}/$tableName.sql\").collect()(0) \n    \n  // Remove the replace for the .dat once it is fixed in the github repo\n  val sqlStmts = content.stripLineEnd\n    .replace('\\n', ' ')\n    .replace(\"${TPCDS_GENDATA_DIR}\", tpcdsGenDataDir)\n    .replace(\"csv\", \"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\").split(\";\")\n  sqlStmts.map(stmt => journey_spark.sql(stmt))    \n}  "
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "execution_count": 7, 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "runQuery: (queryNum: Int)Unit\n"
                    }
                }
            ], 
            "source": "def runQuery(queryNum: Int): Unit = {\n  val queryStr = \"%02d\".format(queryNum)  \n  val queryName = s\"${tpcdsQueriesDir}/query${queryStr}.sql\"   \n  val (_, content) = journey_spark.sparkContext.wholeTextFiles(queryName).collect()(0)  \n  val queries = content.split(\"\\n\")\n    .filterNot (_.startsWith(\"--\"))\n    .mkString(\" \").split(\";\")\n  println(s\"Running TPC-DS Query $queryName\")\n  for (query <- queries)  {\n   val start = System.nanoTime()\n   val result = journey_spark.sql(query).collect  \n   val timeElapsed = (System.nanoTime() - start) / 1000000000\n   totalTime = totalTime + timeElapsed\n    // result.foreach(println) \n   println(s\"Time elapsed: $timeElapsed, Number of rows: ${result.length}\") \n  }\n}"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### Create the database and tables.\n* Creates the tpc-ds database.\n* For each of the table name in TPC-DS schema, calls up on the function to create the table in spark."
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "execution_count": 8, 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "Finish creating database TPCDS2G\nCreating table call_center ..\nCreating table catalog_sales ..\nCreating table customer_demographics ..\nCreating table income_band ..\nCreating table promotion ..\nCreating table store ..\nCreating table time_dim ..\nCreating table web_returns ..\nCreating table catalog_page ..\nCreating table customer ..\nCreating table date_dim ..\nCreating table inventory ..\nCreating table reason ..\nCreating table store_returns ..\nCreating table warehouse ..\nCreating table web_sales ..\nCreating table catalog_returns ..\nCreating table customer_address ..\nCreating table household_demographics ..\nCreating table item ..\nCreating table ship_mode ..\nCreating table store_sales ..\nCreating table web_page ..\nCreating table web_site ..\n", 
                    "output_type": "stream"
                }, 
                {
                    "metadata": {}, 
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "tables = Array(call_center, catalog_sales, customer_demographics, income_band, promotion, store, time_dim, web_returns, catalog_page, customer, date_dim, inventory, reason, store_returns, warehouse, web_sales, catalog_returns, customer_address, household_demographics, item, ship_mode, store_sales, web_page, web_site)\n"
                    }
                }, 
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "[call_center, catalog_sales, customer_demographics, income_band, promotion, store, time_dim, web_returns, catalog_page, customer, date_dim, inventory, reason, store_returns, warehouse, web_sales, catalog_returns, customer_address, household_demographics, item, ship_mode, store_sales, web_page, web_site]"
                    }, 
                    "execution_count": 8
                }
            ], 
            "source": "// TPC-DS table names.\nval tables = Array(\"call_center\", \"catalog_sales\",\n                   \"customer_demographics\", \"income_band\",\n                   \"promotion\", \"store\", \"time_dim\", \"web_returns\",\n                   \"catalog_page\", \"customer\", \"date_dim\",\n                   \"inventory\", \"reason\", \"store_returns\", \"warehouse\",\n                   \"web_sales\", \"catalog_returns\", \"customer_address\",\n                   \"household_demographics\", \"item\", \"ship_mode\", \"store_sales\",\n                   \"web_page\", \"web_site\" )\n\n// Create database\ncreateDatabase\n\n// Create table\nfor ( table <- tables) {\n    try {\n      createTable(table)\n    } catch {\n      case e: Throwable => {\n        println(\"EXCEPTION!! \" + e.getMessage())\n        throw e\n      }\n    }\n}\n"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "## Verify table creating and data loading.\n* Run a simple Spark SQL query to get the count of rows\n* Verify that the row counts are as expected"
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "execution_count": 9, 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "=====================================================\nLoaded and verified the table counts successfully\n=====================================================\n", 
                    "output_type": "stream"
                }, 
                {
                    "metadata": {}, 
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "rowCounts = Array(6, 1441548, 1920800, 20, 300, 12, 86400, 71763, 11718, 100000, 73049, 11745000, 35, 287514, 5, 719384, 144067, 50000, 7200, 18000, 20, 2880404, 60, 30)\nexpectedCounts = Array(6, 1441548, 1920800, 20, 300, 12, 86400, 71763, 11718, 100000, 73049, 11745000, 35, 287514, 5, 719384, 144067, 50000, 7200, 18000, 20, 2880404, 60, 30)\nerrorCount = 0\nzippedCountsWithIndex = Array(((6,6),0), ((1441548,1441548),1), ((1920800,1920800),2), ((20,20),3), ((300,300),4), ((12,12),5), ((86400,86400),6), ((71763,71763),7), ((11718,11718),8), ((100000,100000),9), ((73049,73049),10), ((11745000,11745000),11), ((35,35),12), ((287514,287514),13), ((5,5),14), ((719384,719384),15), ((144067,144067),16), ((50000,50000),17), ((7200,7200),18)...\n"
                    }
                }, 
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "[((6,6),0), ((1441548,1441548),1), ((1920800,1920800),2), ((20,20),3), ((300,300),4), ((12,12),5), ((86400,86400),6), ((71763,71763),7), ((11718,11718),8), ((100000,100000),9), ((73049,73049),10), ((11745000,11745000),11), ((35,35),12), ((287514,287514),13), ((5,5),14), ((719384,719384),15), ((144067,144067),16), ((50000,50000),17), ((7200,7200),18), ((18000,18000),19), ((20,20),20), ((2880404,2880404),21), ((60,60),22), ((30,30),23)]"
                    }, 
                    "execution_count": 9
                }
            ], 
            "source": "// Run a count query and get the counts\nval rowCounts = tables.map { table =>\n    journey_spark.table(table).count()\n}\n\nval expectedCounts = Array (\n    6, 1441548, 1920800, 20, 300, 12, 86400,\n    71763,  11718, 100000, 73049, 11745000, \n    35, 287514, 5, 719384, 144067, 50000, 7200,\n    18000, 20, 2880404, 60, 30\n)\n\nvar errorCount = 0;\nval zippedCountsWithIndex = rowCounts.zip(expectedCounts).zipWithIndex\nfor ((pair, index) <- zippedCountsWithIndex) {\n    if (pair._1 != pair._2) {\n        println(s\"\"\"ERROR!! Row counts for ${tables(index)} does not match.\n        Expected=${expectedCounts(index)} but found ${rowCounts(index)}\"\"\")\n        errorCount += 1\n    }\n}\n\nprintln(\"=====================================================\")\nif ( errorCount > 0) {\n  println(s\"Load verification failed with $errorCount errors\")\n} else {\n  println(\"Loaded and verified the table counts successfully\")\n}\nprintln(\"=====================================================\")"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "## Run a query by its number\n* Given a query number between 1 to 99, run it against spark.\n* Display the query results, time taken to execute the query and number of rows returned.\n* To run a differnt query , please change the QUERY_NUM to a valid value from 1 to 99."
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "execution_count": 10, 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "Running TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query32.sql\nTime elapsed: 21, Number of rows: 1\n", 
                    "output_type": "stream"
                }, 
                {
                    "metadata": {}, 
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "QUERY_NUM = 32\n"
                    }
                }, 
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "32"
                    }, 
                    "execution_count": 10
                }
            ], 
            "source": "val QUERY_NUM = 32\nrunQuery(QUERY_NUM)"
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "execution_count": null, 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "Running TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query01.sql\nTime elapsed: 40, Number of rows: 100\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query02.sql\nTime elapsed: 19, Number of rows: 2513\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query03.sql\nTime elapsed: 11, Number of rows: 89\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query04.sql\nTime elapsed: 52, Number of rows: 8\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query05.sql\nTime elapsed: 17, Number of rows: 100\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query06.sql\nTime elapsed: 15, Number of rows: 45\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query07.sql\nTime elapsed: 9, Number of rows: 100\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query08.sql\nTime elapsed: 7, Number of rows: 5\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query09.sql\nTime elapsed: 4, Number of rows: 1\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query10.sql\nTime elapsed: 17, Number of rows: 5\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query11.sql\nTime elapsed: 23, Number of rows: 88\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query12.sql\nTime elapsed: 7, Number of rows: 100\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query13.sql\nTime elapsed: 12, Number of rows: 1\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query14.sql\nTime elapsed: 54, Number of rows: 100\nTime elapsed: 41, Number of rows: 100\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query15.sql\nTime elapsed: 12, Number of rows: 100\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query16.sql\nTime elapsed: 11, Number of rows: 1\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query17.sql\nTime elapsed: 14, Number of rows: 1\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query18.sql\nTime elapsed: 10, Number of rows: 100\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query19.sql\nTime elapsed: 5, Number of rows: 100\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query20.sql\nTime elapsed: 10, Number of rows: 100\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query21.sql\nTime elapsed: 10, Number of rows: 100\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query22.sql\nTime elapsed: 9, Number of rows: 100\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query23.sql\nTime elapsed: 34, Number of rows: 1\nTime elapsed: 25, Number of rows: 4\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query24.sql\nTime elapsed: 20, Number of rows: 0\nTime elapsed: 16, Number of rows: 0\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query25.sql\nTime elapsed: 8, Number of rows: 1\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query26.sql\nTime elapsed: 5, Number of rows: 100\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query27.sql\nTime elapsed: 5, Number of rows: 100\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query28.sql\nTime elapsed: 5, Number of rows: 1\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query29.sql\nTime elapsed: 8, Number of rows: 1\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query30.sql\nTime elapsed: 11, Number of rows: 100\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query31.sql\nTime elapsed: 13, Number of rows: 51\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query32.sql\nTime elapsed: 5, Number of rows: 1\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query33.sql\nTime elapsed: 9, Number of rows: 100\n=====================================================\nAll TPC-DS queries ran successfully\nTotal Elapsed Time so far: 594 seconds.\n=====================================================\n", 
                    "output_type": "stream"
                }, 
                {
                    "metadata": {}, 
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "queryErrors = 0\n"
                    }
                }, 
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "0"
                    }, 
                    "execution_count": 11
                }
            ], 
            "source": "var queryErrors = 0\nfor (i <- 1 to 33) {\n  try{\n    runQuery(i)\n  } catch {\n     case e: Throwable => {\n          println(\"Error in query \"+ i + \"msg = \" + e.getMessage)\n          queryErrors += 1\n     }\n  }\n}\nprintln(\"=====================================================\")\nif ( queryErrors > 0) {\n  println(s\"Query execution failed with $queryErrors errors\")\n} else {\n  println(\"All TPC-DS queries ran successfully\")\n}\nprintln (s\"Total Elapsed Time so far: ${totalTime} seconds.\")\nprintln(\"=====================================================\")"
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "execution_count": null, 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "Running TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query34.sql\nTime elapsed: 6, Number of rows: 451\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query35.sql\nTime elapsed: 23, Number of rows: 100\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query36.sql\nTime elapsed: 9, Number of rows: 100\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query37.sql\nTime elapsed: 3, Number of rows: 1\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query38.sql\nTime elapsed: 13, Number of rows: 1\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query39.sql\nTime elapsed: 19, Number of rows: 246\nTime elapsed: 16, Number of rows: 17\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query40.sql\nTime elapsed: 11, Number of rows: 100\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query41.sql\nTime elapsed: 3, Number of rows: 4\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query42.sql\nTime elapsed: 7, Number of rows: 10\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query43.sql\nTime elapsed: 8, Number of rows: 6\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query44.sql\nTime elapsed: 8, Number of rows: 10\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query45.sql\nTime elapsed: 19, Number of rows: 19\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query46.sql\nTime elapsed: 15, Number of rows: 100\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query47.sql\nTime elapsed: 41, Number of rows: 100\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query48.sql\nTime elapsed: 10, Number of rows: 1\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query49.sql\nTime elapsed: 17, Number of rows: 32\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query50.sql\nTime elapsed: 12, Number of rows: 6\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query51.sql\nTime elapsed: 30, Number of rows: 100\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query52.sql\nTime elapsed: 7, Number of rows: 100\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query53.sql\nTime elapsed: 13, Number of rows: 100\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query54.sql\nTime elapsed: 23, Number of rows: 1\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query55.sql\nTime elapsed: 6, Number of rows: 100\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query56.sql\nTime elapsed: 23, Number of rows: 100\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query57.sql\nTime elapsed: 44, Number of rows: 100\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query58.sql\nTime elapsed: 27, Number of rows: 3\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query59.sql\nTime elapsed: 17, Number of rows: 100\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query60.sql\nTime elapsed: 28, Number of rows: 100\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query61.sql\nTime elapsed: 28, Number of rows: 1\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query62.sql\nTime elapsed: 17, Number of rows: 100\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query63.sql\nTime elapsed: 14, Number of rows: 100\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query64.sql\nTime elapsed: 154, Number of rows: 10\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query65.sql\nTime elapsed: 33, Number of rows: 100\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query66.sql\nTime elapsed: 54, Number of rows: 5\n=====================================================\nAll TPC-DS queries ran successfully\nTotal Elapsed Time so far: 1352 seconds.\n=====================================================\n", 
                    "output_type": "stream"
                }, 
                {
                    "metadata": {}, 
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "queryErrors = 0\n"
                    }
                }, 
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "0"
                    }, 
                    "execution_count": 12
                }
            ], 
            "source": "var queryErrors = 0\nfor (i <- 34 to 66) {\n  try{\n    runQuery(i)\n  } catch {\n     case e: Throwable => {\n          println(\"Error in query \"+ i )\n          queryErrors += 1\n     }\n  }\n}\nprintln(\"=====================================================\")\nif ( queryErrors > 0) {\n  println(s\"Query execution failed with $queryErrors errors\")\n} else {\n  println(\"All TPC-DS queries ran successfully\")\n}\nprintln (s\"Total Elapsed Time so far: ${totalTime} seconds.\")\nprintln(\"=====================================================\")"
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "execution_count": null, 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "Running TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query67.sql\nTime elapsed: 31, Number of rows: 100\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query68.sql\nTime elapsed: 32, Number of rows: 100\nTime elapsed: 41, Number of rows: 100\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query70.sql\nTime elapsed: 43, Number of rows: 3\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query71.sql\nTime elapsed: 44, Number of rows: 1018\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query72.sql\nTime elapsed: 175, Number of rows: 100\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query73.sql\nTime elapsed: 57, Number of rows: 5\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query74.sql\nTime elapsed: 122, Number of rows: 92\nRunning TPC-DS Query /user/clsadmin/tpcds-journey/src/queries/query75.sql\nException in thread \"dispatcher-event-loop-3\" java.lang.OutOfMemoryError: GC overhead limit exceeded\n\tat java.lang.Class.getDeclaredMethods0(Native Method)\n\tat java.lang.Class.privateGetDeclaredMethods(Class.java:2701)\n\tat java.lang.Class.getDeclaredMethod(Class.java:2128)\n\tat java.io.ObjectStreamClass.getInheritableMethod(ObjectStreamClass.java:1442)\n\tat java.io.ObjectStreamClass.access$2200(ObjectStreamClass.java:72)\n\tat java.io.ObjectStreamClass$2.run(ObjectStreamClass.java:510)\n\tat java.io.ObjectStreamClass$2.run(ObjectStreamClass.java:472)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.io.ObjectStreamClass.<init>(ObjectStreamClass.java:472)\n\tat java.io.ObjectStreamClass.lookup(ObjectStreamClass.java:369)\n\tat java.io.ObjectStreamClass.<init>(ObjectStreamClass.java:468)\n\tat java.io.ObjectStreamClass.lookup(ObjectStreamClass.java:369)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1134)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n\tat java.io.ObjectOutputStream.defaultWriteObject(ObjectOutputStream.java:441)\n\tat org.apache.spark.rdd.ZippedPartitionsPartition$$anonfun$writeObject$1.apply$mcV$sp(ZippedPartitionsRDD.scala:41)\n\tat org.apache.spark.rdd.ZippedPartitionsPartition$$anonfun$writeObject$1.apply(ZippedPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.ZippedPartitionsPartition$$anonfun$writeObject$1.apply(ZippedPartitionsRDD.scala:38)\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1280)\n\tat org.apache.spark.rdd.ZippedPartitionsPartition.writeObject(ZippedPartitionsRDD.scala:38)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1028)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n", 
                    "output_type": "stream"
                }
            ], 
            "source": "var queryErrors = 0\nfor (i <- 67 to 99) {\n  try{\n    runQuery(i)\n  } catch {\n     case e: Throwable => {\n          println(\"Error in query \"+ i )\n          queryErrors += 1\n     }\n  }\n}\nprintln(\"=====================================================\")\nif ( queryErrors > 0) {\n  println(s\"Query execution failed with $queryErrors errors\")\n} else {\n  println(\"All TPC-DS queries ran successfully\")\n}\nprintln (s\"Total Elapsed Time so far: ${totalTime} seconds.\")\nprintln(\"=====================================================\")"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Scala 2.11 with Spark 2.1", 
            "name": "scala-spark21", 
            "language": "scala"
        }, 
        "language_info": {
            "mimetype": "text/x-scala", 
            "name": "scala", 
            "pygments_lexer": "scala", 
            "version": "2.11.8", 
            "file_extension": ".scala", 
            "codemirror_mode": "text/x-scala"
        }
    }, 
    "nbformat_minor": 1, 
    "nbformat": 4
}